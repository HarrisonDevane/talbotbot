# Talbotbot ðŸ˜ˆ

**Version:** 1.5  
**Rating:** ~1700 Lichess Elo  
**Engine Type:** Chess AI using MCTS + CNN  

Talbotbot is a neural-network-driven chess engine that combines **Monte Carlo Tree Search (MCTS)** with a **Convolutional Neural Network (CNN)** to evaluate positions and play moves.

## Demo

Watch Talbotbot in action or play games at its Lichess profile:  
**https://lichess.org/@/Talbotbot**

## Model Architecture

Talbotbot v1.5 uses a deep convolutional neural network (CNN) inspired by AlphaZero, with a residual architecture tailored for chess board evaluation. The model is trained to output both a move policy and a value estimate from a given board state.

---

### Input

- **Shape:** `(batch_size, 68, 8, 8)`
- **Encoding:** 68 input planes encode the full game state, including:
  - 12 planes for the piece types for both players
  - 4 planes for castling rights
  - 1 plane for the current turn
  - 1 plane for en passant
  - 2 planes for repetition history
  - 12 x 4 planes for the previous 4 board states

---

### Core Network

- 1 initial `3Ã—3` convolution with batch normalization and ReLU
- **20 Residual Blocks**, each with:
  - Two `3Ã—3` convolutions
  - Batch normalization + ReLU
  - Residual skip connection
  - **Dropout2D** applied in blocks 10 through 20 (`dropout_rate_conv = 0.1`)

---

### Policy Head

- `1Ã—1` convolution â†’ BatchNorm â†’ ReLU  
- Fully connected layer outputs logits for **8Ã—8Ã—73 = 4672** possible moves  
- Used to guide MCTS with softmax sampling

---

### Value Head

- `1Ã—1` convolution â†’ BatchNorm â†’ ReLU  
- Fully connected â†’ Dropout â†’ Fully connected  
- Final scalar output passed through `tanh`, representing:
  - **+1** = certain win  
  - **0** = draw  
  - **âˆ’1** = certain loss

---

### Summary Table

| Component         | Description                                        |
|------------------|----------------------------------------------------|
| Input             | `(batch_size, 68, 8, 8)`                           |
| Residual Blocks   | 20 Ã— [Conv3Ã—3 â†’ BN â†’ ReLU â†’ Dropout (optional)]   |
| Dropout Strategy  | Dropout2D from block 5 onward                     |
| Policy Output     | 4672-class logits (8Ã—8Ã—73)                         |
| Value Output      | Scalar in `[-1, 1]` via `tanh`                     |

---

## Training Data

Talbotbot v1.5 was trained using supervised learning on labeled datasets sourced from human and engine play. The next major release, Talbotbot v2, will shift toward reinforcement learning (RL) via self-play and policy iteration.

---

### Data Strategy

To ensure strong generalization and position understanding, training data is drawn from a variety of sources covering strategic, tactical, and endgame play. Each position is labeled with a Stockfish evaluation and, the human played (or principal variation for endgames) moves for policy supervision.

---

### Dataset Overview

| Source                   | Description                                                               | Volume            |
|--------------------------|---------------------------------------------------------------------------|-------------------|
| Grandmaster Games        | 20 million positions (30+ moves each)                                    | ~20 million positions  |
| Lichess Elite            | 5 million positions by players rated 2600+, filtered to 30+ move games        | ~5 million positions |
| Tactics Corpus           | High-quality tactical positions extracted from puzzles and sharp games    | ~2 million         |
| Synthetic Endgames       | Random legal 3â€“7 piece positions generated by sampling and filtering      | ~2 million         |

---

### Position Reconstruction

For snapshot-based datasets (e.g., tactics and synthetic endgames), Talbotbot reconstructs prior context by playing **4 legal moves** into the position. This ensures accurate board state encoding including repetition, castling rights, and move counters.

---

### Labeling and Evaluation

- **Stockfish Evaluation:**  
  All positions are evaluated using Stockfish with a time budget of **0.02 seconds** per position. This provides scalar value labels based on the best move sequence.

- **Policy Labels:**  
  Where possible, policy labels are derived from the principal variation of the engine (endgame) or the move played in the original game (for supervised datasets).

---

### Synthetic Endgame Generation

Synthetic endgames were generated using controlled random sampling of legal positions involving 3â€“7 pieces. Generation rules include:

- Exclusion of illegal/impossible configurations (e.g., too many pawns or duplicate kings)
- Balance of piece types to avoid triviality (e.g., K vs K)

---

### Transition to Reinforcement Learning

While Talbotbot v1.5 relies entirely on supervised learning, **Talbotbot v2 will use reinforcement learning**, generating its own training data via self-play and MCTS-guided exploration. This will enable stronger policy generalization and long-term strategic planning.